---
title: "DBZ_latimes_milestone02"
author: "Zaka Hossain, David Landerman, Bex Heimbrock"
date: "2025-10-11"
output: html_document
---

Work division: David wrote the code to pull and clean the data, Bex wrote and ran the code for chosen paragraph #1; David wrote and ran the code for chosen paragraph #2; and Zaka reorganized, validated and consolidated the code, afterwards Bex and David went through to check their respective sections. 

#Analysis: Bex Heimbrock, Zaka Hossain, David Landerman

#Note to RW: As this is still a draft, we will note questions to you in this format

L.A. is slammed with record costs for legal payouts:
https://www.latimes.com/local/lanow/la-me-ln-city-payouts-20180627-story.html
By Emily Alpert Reyes and Ben Welsh
GitHub repo for story with data:
https://github.com/datadesk/la-settlements-analysis

##Paragraphs Chosen (not in order of importance):

1. Last budget year, the city paid out more than $200 million in legal settlements and court judgments — a record amount that was more than the city spent on its libraries or fixing its streets.

2. Beyond the mammoth cases, a pileup of other large payouts has padded the tab, a Times analysis of city payouts found. While the total number of payouts has dipped over time, the average amount has increased. Last budget year, the city paid out 30 settlements of $1 million or more — five times as many as a decade before.
3. Thousands of legal battles involving the Los Angeles Police Department, including a rash of recent settlements over police shootings and wrongful imprisonment, have added up to more than 40% of the total — a higher sum than any other city department, the analysis showed. That includes more than $24 million paid to two men who were freed after decades in prison, along with lawsuits over arrests, injuries, car crashes and alleged retaliation and discrimination against its own employees.
4. Legal payouts for this budget year are estimated to total around $109 million. Although the city is spending less on payouts this year than last, it also has been saddling future budgets with payout costs, committing up to $16 million next budget year to help pay off settlements that it already has approved.
5. “Both Feuer and Garcetti stressed that the surge is tied to a few cases with extraordinarily large payouts: Last budget year, a class-action lawsuit over a telephone tax cost the city $45 million. In another case, more than $23 million went to the family of a motorcyclist killed at a San Pedro intersection. And the city spent an additional $21 million last budget year in a case alleging that L.A. failed to make sure enough publicly funded housing was accessible to people with disabilities.”
 
##Rationale

What Makes These 5 data driven and important:

1. It is a clear nut graph and is about the broad point of total money paid out from court settlements. By comparing payout figures to expenditure on libraries and “fixing its streets,” it provides context for readers – a much needed anchor point.

2. “Time’s analysis of city payouts found” is a dead giveaway for data driven paragraphs. It talks about the totals by year as well as the average by year. The paragraph talks not only about a fall in the quantity of settlements but also the rise in the value of each of the settlements as a reason for overall increase in the total value. Up top, it also makes it clear that these figures do not include outliers mentioned in the previous paragraph(s).  

3. “Thousands of legal battles” and talking about 40% of the total payouts are data driven statements. It also compared the total dollar amount for police settlements vs settlements resulting from other departments. Highlighting the percentage share, it identifies LAPD as the department using up a major chunk of the payouts share. 

4. This is important because it talks about the fiscal reality of payouts paid in installments. And gives us an idea about the downward trend, and estimation determined from that.

5. This paragraph talks about outliers in the data taking up the lionshare of the massive number presented in the nut graph.

##Why these paragraphs were more important than other data driven paragraphs:
Other paragraphs compared the city’s settlement numbers to that of other cities as well as an increase in bicycle crash payments.These are important statistical comparisons, especially the one which put Los Angeles in the context of other cities, however they were not the main focus of the story and were therefore de-prioritized. Another shorter paragraph gave the high level statistic in the nut graph slightly more definition. Though important, this paragraph was not in the top 5 due to its repetitive nature. In general, the chosen paragraphs demonstrate clear, original reporting based on data pulled from records requests and other sources. 

#Zaka comment: The sections of Bex and David's work have been slightly reformatted and edited for consistency

#START David's work: Pulling/cleaning data

##Note to RW: are we supposed to understand in general what the following code chunk means or why it's important? I just copy and paste from the labs.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
```

##loading libaries here
```{r}
library(tidyverse)
library(rio)
library(lubridate)
library(janitor)
```

# Loading Data
```{r}
#import data function
payouts <- rio::import('payouts.xlsx', 
                       skip=1, header=TRUE)
#The above skip/header functions deletes the wonky top row, make the second row into headers
#See AI Disclosure I


payouts <- payouts %>% 
  slice_head(n=18701) %>% 
  clean_names()
#the bottom two rows in the csv are one blank row and then a row with a grand total, this will screw up the data. Slice_head means filtering by that number of rows, I tried to use slice_tail(n=2) but it gave me JUST the bottom two rows, so I did total rows minus 2 and slice_head.
##Note to RW if you have a suggestion on a more succinct/elegant way to achieve the above function please let us know

#BROUGHT INTO DAVIDS SECTION FROM BEX's CODE
#Load department totals for comparison
dept_totals <- rio::import('department-totals.csv')
```

*David's AI DISCLOSURE I (for import)*
I Googled "is there a way to import a csv into r and delete the top row and use row 2 as the header"
The first link was substack, I read it, not helpful but the top result was google's ai which said this
"Yes, it is possible to import a CSV file into R, skip the first row, and use the second row as the header. This can be achieved using the read.csv() function from base R or the read_csv() function from the readr package.
Using read.csv() (Base R):
Code

data <- read.csv("your_file.csv", skip = 1, header = TRUE)"
*END AI DISCLOSURE I*

#END David's work: Pulling/cleaning data (Started line 48)

#START Bex's Work: Paragraph 1 Reverse Engineering

##Paragraph 1: Total Payouts vs City Spending 

**Target Text:** "Last budget year, the city paid out more than $200 million in legal settlements and court judgments — a record amount that was more than the city spent on its libraries or fixing its streets."

## Calculate yearly totals
**Finding 1.1** : The 2017 fiscal year had total payouts of $201,372,336. 
```{r}
yearly_data_sum <- payouts %>% 
  group_by(fiscal_year) %>%
  summarise(
    total_cases = n(),
    total_amount = sum(amount, na.rm = TRUE)
  ) %>% 
  arrange(fiscal_year)

# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>% 
  filter(fiscal_year == 2017)
```
Here was my original code, which did everything I wanted except combine all instances of X year to make one data column per year: 
year_amount <- payouts |> 
group_by(fiscal_year, amount) |> 
summarise(total_amount=sum(amount)) |> 
arrange(desc(amount))
That didn't work, so...
I looked back through my R notebook and found only stringer detect functions.
This is what I asked Claude: How to combine all instances of a year being mentioned in a spreadsheet? Claude then asked me to explain whether the data was in its own column, and how it was formatted.
Claude suggested the above code with the following explanatory comment: #total_cases: number of payments made that year
total_amount: sum of all payments
avg_amount: average payment amount
min_amount and max_amount: smallest and largest payments
Thanks, Claude, but that's kind of a lot - I just want to know the total amount. So, all I included was total_amount.

So, the total for 2018 is $72,829,597... that doesn't seem right. LA Times says the city paid out $200million! But, the total for 2017 is over $201million. This article was published in 2018. So, last budget year was, in fact, 2017. Yay!

Now it's time to read in the csv file containing totals spent on public works.

## Compare to library and public works spending
Hey R, how much was spent on public works and libraries? 
**Finding 1.2** This exceeds the library budget 3,433,873.
**Zaka's Note**: The LA Times article states settlements exceeded what the city spent on "fixing its streets" - this may refer to a specific portion of the Public Works budget rather than the entire department budget. Street repair/maintenance would likely be a subset of the total Public Works spending.
```{r}
# Filter for PUBLIC WORKS and LIBRARY departments
comparison_depts <- dept_totals %>%
  filter(str_detect(latimes_department, 'PUBLIC WORKS|LIBRARY')) %>%
  arrange(desc(amount))

comparison_depts
```
This was my initial code...
> cherry_picking_data_for_maximum_effect <- dept_totals |>
#filter(str_detect(latimes_department, 'PUBLIC WORKS', 'LIBRARY))

That didn't work. I went back to pre-lab03 and tried to string together that code, this is what I ended up with...
> cherry_picking_data_for_maximum_effect <- dept_totals |>
#sort_by(amount) |>
#filter(str_detect('PUBLIC WORKS', 'LIBRARY')) |>
#arrange(desc(amount))

That also didn't work. So then I asked Claude... 
Why isn't this code working: cherry_picking_data_for_maximum_effect <- dept_totals |>
 filter(str_detect('PUBLIC WORKS', 'LIBRARY')) |>
 arrange(desc(amount))

Claude told me to fix three things...
First argument should be the column name (like department) - not in quotes
Use the | (OR) operator to find either "PUBLIC WORKS" or "LIBRARY"
Make sure you're using the correct column name from your dataframe

So that's how I got the above data.
#END Bex's Work: Paragraph 1 Reverse Engineering (Started Line 97)

#David's Work: Paragraph 2 Reverse Engineering

##Paragraph 2: Payout Trends Analysis

**Target Text:** "Beyond the mammoth cases, a pileup of other large payouts has padded the tab, a Times analysis of city payouts found. While the total number of payouts has dipped over time, the average amount has increased. Last budget year, the city paid out 30 settlements of $1 million or more — five times as many as a decade before."

## Create comprehensive summary table
#the following code chunk was developed independently from but is slightly redundant to the calculation of the dataframe called yearly_data. Please let us know if you would like us to combine the code chunks moving forward.
```{r}
payout_trends_summary <- payouts %>%
  group_by(fiscal_year) %>%
  summarize(
    number_of_cases_in_each_year = n(),
    total_for_year = sum(amount, na.rm=TRUE),
    average_payout = mean(amount, na.rm=TRUE),
      #then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table *SEE AI DISCLOSURE 2.1, for troubleshooting of the following line* 
    number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
  )
```

## Finding 2.1: Total number of payouts has dipped over time
```{r}
payout_trends_summary %>%
    #SEE AI DISCLOSURE 2.2 for troubleshooting of the following line only (if I am using AI to troubleshoot too much please let me know, for the record I am attempting, then googling, then if I cant find/understand what I've found I go to CLAUDE) 
  mutate(
    pct_change_from_baseline = ((number_of_cases_in_each_year - first(number_of_cases_in_each_year))/
                   first(number_of_cases_in_each_year)) * 100,
    difference_from_baseline = number_of_cases_in_each_year - first(number_of_cases_in_each_year)
  ) %>%
  select(fiscal_year, number_of_cases_in_each_year, difference_from_baseline, pct_change)
#NOTE TO RW: trends difference from article suggest missing data cleaning step, search "DATA CLEANING INVESTIGATION" in this code for what I'd like your thoughts on
#NOTE TO RW: Is the difference from baseline and pct change from baseline helpful? Or would you recommend changing it to year to year difference/pct change?
```
**Zaka's Note**: The output agrees with the proposed statement above, if the period is taken from 2013 to 2018 with a minor exception in 2017.

## Finding 2.2: Average amount has increased

```{r}
payout_trends_summary %>%
  mutate(
    pct_change_from_baseline = ((average_payout - first(average_payout))/first(average_payout)) * 100,
    difference_from_baseline = average_payout - first(average_payout)
  ) %>%
  select(fiscal_year, average_payout, difference_from_baseline, pct_change)
```
**Zaka's Note**: The output agrees with the proposed statement above, if the period is taken from 2013 to 2017.

## Finding 2.3: 30 settlements of $1 million or more in 2017

```{r}
payout_trends_summary %>%
  filter(fiscal_year == 2017) %>%
  select(fiscal_year, number_of_cases_over_one_million)
```
**Zaka's Note**: The output agrees with the proposed statement above. 30 cases over 1M.

## Finding 2.4: Five times as many as a decade before

```{r}
payout_trends_summary %>%
  filter(fiscal_year == 2007 | fiscal_year == 2017) %>%
  mutate(decade_change_multiplier = number_of_cases_over_one_million/
           first(number_of_cases_over_one_million)) %>%
  select(fiscal_year, number_of_cases_over_one_million, decade_change_multiplier)
#NOTE TO RW: For clarities sake should I remove the "Decade cahnge multiplier" because maybe it implies that 2007 was the same as 1997. And instead just do a math operation outside the dataframe.
```
**Zaka's Note**: The output agrees with the proposed statement above. Personally would've eyeballed it but David's dedicated to coding it out. I am glad he is on my team.

##More from David

AI DISCLOSURES

*AI USE DISCLOSURE 2.1*

I tried the code in the prompt and got an error related to trying to filter inside an n(). I found it hard to figure out exactly what I was looking for online so I asked Claude a very specific prompt so it wouldn't over-explain/make this too easy on me.
David: In R with the following libraries loaded (tidyverse, rio, lubridate, janitor) and the following code run: 

#```{r}
payouts %>% 
  group_by(fiscal_year) %>% 
  summarize(
    number_of_cases_in_each_year=n(),
    total_for_year=sum(amount,na.rm=TRUE),
    average_payout=mean(amount,na.rm=TRUE),
    #then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table
    number_of_cases_over_one_million=n(filter(amount>=1000000))
  )
'''

Generating the following error:
Error in summarize(): ℹ In argument: number_of_cases_over_one_million = n(filter(amount >= 1e+06)). ℹ In group 1: fiscal_year = 2006. Caused by error in n(): ! unused argument (filter(amount >= 1e+06)) Run rlang::last_trace() to see where the error occurred.
Considering the comment that begins with "#then filter the rows in the source table" explaining the intent of the following line, QUESTION: What went wrong with how I wrote the code in that line?

RESPONSE GUIDELINE: Be as succinct as possible, do not over-explain. Give me the corrected LINE of code (not the whole chunk) and one sentence about why what I did was wrong and THAT IS IT!

CLAUDE: r
number_of_cases_over_one_million=sum(amount >= 1000000, na.rm=TRUE)

You tried to pass filter() as an argument to n(), but n() counts all rows in the group without arguments—use sum() with a logical condition instead to count rows meeting your criteria.
DAVID: wouldn't sum give me the sum of all the amounts in those rows of over 1000000, I want to know how many rows exist with amounts of 1 million. Hypothetically if there are 10 rows in a given fiscal year with each having exactly 1 million for amount then I want this code to generate the number 10 for that row in the pivot table not 10 million. Will what you gave me generate 10 or 10 million for the value in the pivot table row?

CLAUDE: It will generate 10, not 10 million.
When you use sum() on a logical condition like amount >= 1000000, R converts TRUE to 1 and FALSE to 0, then sums those 1s and 0s, giving you the count of TRUE values (i.e., the number of rows meeting the condition).

DAVID: Ok so conversely if I wanted to generate 10 million from that example how would I do that? If I wanted the sum of the amounts for all the rows over 1 million?

CLAUDE: r
sum_of_cases_over_one_million=sum(amount[amount >= 1000000], na.rm=TRUE)
Use bracket subsetting amount[amount >= 1000000] to filter the amounts first, then sum() adds up those filtered values.

DAVID: are you sure its brackets not parenthesis? If so why?

CLAUDE: Yes, brackets [ ] not parentheses ( ).
Brackets are R's subsetting operator that extracts elements from a vector based on a logical condition, while parentheses are for function calls—amount[amount >= 1000000] means "give me the values from amount where the condition is TRUE."
*END AI DISCLOSURE*

*AI DISCLOSURE 2.2*
DAVID:For another purpose, subtracting  each value in the column number_of_cases_in_each_year by the first value (which corresponds to fiscal year 2016) I tried this code and the column it returned had values that were all NA. What did I do wrong? be succinct
payout_trends_summary %>% 
  mutate(difference_from_baseline=number_of_cases_in_each_year-number_of_cases_in_each_year[fiscal_year=2006]) %>% 
  select(fiscal_year,number_of_cases_in_each_year,difference_from_baseline)

CLAUDE: r
difference_from_baseline=number_of_cases_in_each_year-number_of_cases_in_each_year[fiscal_year==2006]
You used = (assignment) instead of == (comparison) inside the brackets, so the condition failed and returned NA.

DAVID: And if I wanted to generalize the first year (i.e. not use 2006)

CLAUDE:r
difference_from_baseline=number_of_cases_in_each_year-first(number_of_cases_in_each_year)
Use first() to grab the first value in the column without hardcoding the year.
*END AI DISCLOSURE 2.2*

DATA CLEANING INVESTIGATION: I noticed the paragraph said "cases dipped over time" but I didn't see that in the data so I looked to make sure I didn't miss a data cleaning step. I went to amounts and sorted by amount to see if I had any NA rows. However I found some large negative values. I first wondered if these were mislabled and I should apply some kind of absolute value filter (all negatives multiply by -1), but then I looked up the case numbers and saw there were duplicates, so these were amounts returned to the city. Then I saw some cases had 3 lines like one that had a 15million, a negative 15million, then another positive 15million. Thought that might make the data wonky, counting one case as three. Think I may have to filter out one negative payment and its corresponding positive payment if they cancel each other out. Better yet, a net amount on each case number. I wonder how I could do that without accidentally combinign casenumber with values of NA from different years.

#END David's Work: Paragraph 2 Reverse Engineering (Started Line 171)
