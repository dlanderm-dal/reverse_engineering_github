knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(tidyverse)
library(rio)
library(lubridate)
library(janitor)
#import data function
payouts <- rio::import('payouts.xlsx',
skip=1, header=TRUE)
#The above skip/header functions deletes the wonky top row, make the second row into headers
#See AI Disclosure I
payouts <- payouts %>%
slice_head(n=18701) %>%
clean_names()
#the bottom two rows in the csv are one blank row and then a row with a grand total, this will screw up the data. Slice_head means filtering by that number of rows, I tried to use slice_tail(n=2) but it gave me JUST the bottom two rows, so I did total rows minus 2 and slice_head.
##Note to RW if you have a suggestion on a more succinct/elegant way to achieve the above function please let us know
#BROUGHT INTO DAVIDS SECTION FROM BEX's CODE
#Load department totals for comparison
dept_totals <- rio::import('department-totals.csv')
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) %>%
arrange(fiscal_year)
# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>%
filter(fiscal_year == 2017)
# Filter for PUBLIC WORKS and LIBRARY departments
comparison_depts <- dept_totals %>%
filter(str_detect(latimes_department, 'PUBLIC WORKS|LIBRARY')) %>%
arrange(desc(amount))
comparison_depts
payout_trends_summary <- payouts %>%
group_by(fiscal_year) %>%
summarize(
number_of_cases_in_each_year = n(),
total_for_year = sum(amount, na.rm=TRUE),
average_payout = mean(amount, na.rm=TRUE),
#then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table *SEE AI DISCLOSURE 2.1, for troubleshooting of the following line*
number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
)
payout_trends_summary %>%
#SEE AI DISCLOSURE 2.2 for troubleshooting of the following line only (if I am using AI to troubleshoot too much please let me know, for the record I am attempting, then googling, then if I cant find/understand what I've found I go to CLAUDE)
mutate(
pct_change_from_baseline = ((number_of_cases_in_each_year - first(number_of_cases_in_each_year))/
first(number_of_cases_in_each_year)) * 100,
difference_from_baseline = number_of_cases_in_each_year - first(number_of_cases_in_each_year)
) %>%
select(fiscal_year, number_of_cases_in_each_year, difference_from_baseline, pct_change)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(tidyverse)
library(rio)
library(lubridate)
library(janitor)
#import data function
payouts <- rio::import('payouts.xlsx',
skip=1, header=TRUE)
#The above skip/header functions deletes the wonky top row, make the second row into headers
#See AI Disclosure I
payouts <- payouts %>%
slice_head(n=18701) %>%
clean_names()
#the bottom two rows in the csv are one blank row and then a row with a grand total, this will screw up the data. Slice_head means filtering by that number of rows, I tried to use slice_tail(n=2) but it gave me JUST the bottom two rows, so I did total rows minus 2 and slice_head.
##Note to RW if you have a suggestion on a more succinct/elegant way to achieve the above function please let us know
#BROUGHT INTO DAVIDS SECTION FROM BEX's CODE
#Load department totals for comparison
dept_totals <- rio::import('department-totals.csv')
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) %>%
arrange(fiscal_year)
# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>%
filter(fiscal_year == 2017)
# Filter for PUBLIC WORKS and LIBRARY departments
comparison_depts <- dept_totals %>%
filter(str_detect(latimes_department, 'PUBLIC WORKS|LIBRARY')) %>%
arrange(desc(amount))
comparison_depts
payout_trends_summary <- payouts %>%
group_by(fiscal_year) %>%
summarize(
number_of_cases_in_each_year = n(),
total_for_year = sum(amount, na.rm=TRUE),
average_payout = mean(amount, na.rm=TRUE),
#then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table *SEE AI DISCLOSURE 2.1, for troubleshooting of the following line*
number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
)
payout_trends_summary %>%
#SEE AI DISCLOSURE 2.2 for troubleshooting of the following line only (if I am using AI to troubleshoot too much please let me know, for the record I am attempting, then googling, then if I cant find/understand what I've found I go to CLAUDE)
mutate(
pct_change_from_baseline = ((number_of_cases_in_each_year - first(number_of_cases_in_each_year))/
first(number_of_cases_in_each_year)) * 100,
difference_from_baseline = number_of_cases_in_each_year - first(number_of_cases_in_each_year)
) %>%
select(fiscal_year, number_of_cases_in_each_year, difference_from_baseline, pct_change_from_baseline)
#NOTE TO RW: trends difference from article suggest missing data cleaning step, search "DATA CLEANING INVESTIGATION" in this code for what I'd like your thoughts on
#NOTE TO RW: Is the difference from baseline and pct change from baseline helpful? Or would you recommend changing it to year to year difference/pct change?
payout_trends_summary %>%
mutate(
pct_change_from_baseline = ((average_payout - first(average_payout))/first(average_payout)) * 100,
difference_from_baseline = average_payout - first(average_payout)
) %>%
select(fiscal_year, average_payout, difference_from_baseline, pct_change_from_baseline)
payout_trends_summary %>%
filter(fiscal_year == 2017) %>%
select(fiscal_year, number_of_cases_over_one_million)
payout_trends_summary %>%
filter(fiscal_year == 2007 | fiscal_year == 2017) %>%
mutate(decade_change_multiplier = number_of_cases_over_one_million/
first(number_of_cases_over_one_million)) %>%
select(fiscal_year, number_of_cases_over_one_million, decade_change_multiplier)
#NOTE TO RW: For clarities sake should I remove the "Decade cahnge multiplier" because maybe it implies that 2007 was the same as 1997. And instead just do a math operation outside the dataframe.
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(tidyverse)
library(rio)
library(lubridate)
library(janitor)
#import data function
payouts <- rio::import('payouts.xlsx',
skip=1, header=TRUE)
#The above skip/header functions deletes the wonky top row, make the second row into headers
#See AI Disclosure I
payouts <- payouts %>%
slice_head(n=18701) %>%
clean_names()
#the bottom two rows in the csv are one blank row and then a row with a grand total, this will screw up the data. Slice_head means filtering by that number of rows, I tried to use slice_tail(n=2) but it gave me JUST the bottom two rows, so I did total rows minus 2 and slice_head.
##Note to RW if you have a suggestion on a more succinct/elegant way to achieve the above function please let us know
#BROUGHT INTO DAVIDS SECTION FROM BEX's CODE
#Load department totals for comparison
dept_totals <- rio::import('department-totals.csv')
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) %>%
arrange(fiscal_year)
# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>%
filter(fiscal_year == 2017)
# Filter for PUBLIC WORKS and LIBRARY departments
comparison_depts <- dept_totals %>%
filter(str_detect(latimes_department, 'PUBLIC WORKS|LIBRARY')) %>%
arrange(desc(amount))
comparison_depts
payout_trends_summary <- payouts %>%
group_by(fiscal_year) %>%
summarize(
number_of_cases_in_each_year = n(),
total_for_year = sum(amount, na.rm=TRUE),
average_payout = mean(amount, na.rm=TRUE),
#then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table *SEE AI DISCLOSURE 2.1, for troubleshooting of the following line*
number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
)
payout_trends_summary %>%
#SEE AI DISCLOSURE 2.2 for troubleshooting of the following line only (if I am using AI to troubleshoot too much please let me know, for the record I am attempting, then googling, then if I cant find/understand what I've found I go to CLAUDE)
mutate(
pct_change_from_baseline = ((number_of_cases_in_each_year - first(number_of_cases_in_each_year))/
first(number_of_cases_in_each_year)) * 100,
difference_from_baseline = number_of_cases_in_each_year - first(number_of_cases_in_each_year)
) %>%
select(fiscal_year, number_of_cases_in_each_year, difference_from_baseline, pct_change_from_baseline)
#NOTE TO RW: trends difference from article suggest missing data cleaning step, search "DATA CLEANING INVESTIGATION" in this code for what I'd like your thoughts on
#NOTE TO RW: Is the difference from baseline and pct change from baseline helpful? Or would you recommend changing it to year to year difference/pct change?
payout_trends_summary %>%
mutate(
pct_change_from_baseline = ((average_payout - first(average_payout))/first(average_payout)) * 100,
difference_from_baseline = average_payout - first(average_payout)
) %>%
select(fiscal_year, average_payout, difference_from_baseline, pct_change_from_baseline)
payout_trends_summary %>%
filter(fiscal_year == 2017) %>%
select(fiscal_year, number_of_cases_over_one_million)
payout_trends_summary %>%
filter(fiscal_year == 2007 | fiscal_year == 2017) %>%
mutate(decade_change_multiplier = number_of_cases_over_one_million/
first(number_of_cases_over_one_million)) %>%
select(fiscal_year, number_of_cases_over_one_million, decade_change_multiplier)
#NOTE TO RW: For clarities sake should I remove the "Decade cahnge multiplier" because maybe it implies that 2007 was the same as 1997. And instead just do a math operation outside the dataframe.
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(tidyverse)
library(rio)
library(lubridate)
library(janitor)
#import data function
payouts <- rio::import('/Users/rebeccaheimbrock/Desktop/jour772/project/la-settlements-analysis/input/payouts.xlsx',
skip=1, header=TRUE)
#import data function
payouts <- rio::import('payouts.xlsx',
skip=1, header=TRUE)
#The above skip/header functions deletes the wonky top row, make the second row into headers
#See AI Disclosure I
payouts <- payouts %>%
slice_head(n=18701) %>%
clean_names()
#the bottom two rows in the csv are one blank row and then a row with a grand total, this will screw up the data. Slice_head means filtering by that number of rows, I tried to use slice_tail(n=2) but it gave me JUST the bottom two rows, so I did total rows minus 2 and slice_head.
##Note to RW if you have a suggestion on a more succinct/elegant way to achieve the above function please let us know
#BROUGHT INTO DAVIDS SECTION FROM BEX's CODE
#Load department totals for comparison
#Deleted this part, will need to create code that builds a table from data in the Input folder, instead of using the output folder.
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) %>%
arrange(fiscal_year)
# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>%
filter(fiscal_year == 2017)
# Filter for PUBLIC WORKS and LIBRARY departments
comparison_depts <- payouts %>%
filter(str_detect(department, 'PUBLIC WORKS|LIBRARY')) %>%
arrange(desc(amount))
comparison_depts
View(comparison_depts)
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE),
average_payout = mean(amount, na.rm=TRUE),
#then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table *SEE AI DISCLOSURE 2.1, for troubleshooting of the following line*
number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
) %>%
arrange(fiscal_year)
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE),
average_payout = mean(amount, na.rm=TRUE),
#then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table
#*SEE AI DISCLOSURE 2.1, for troubleshooting*
number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
) %>%
arrange(fiscal_year)
yearly_data_sum %>%
#SEE AI DISCLOSURE 2.2 for troubleshooting of the following line only (if I am using AI to troubleshoot too much please let me know, for the record I am attempting, then googling, then if I cant find/understand what I've found I go to CLAUDE)
mutate(
pct_change_from_baseline = ((total_cases - first(total_cases))/
first(total_cases)) * 100,
difference_from_baseline = total_cases - first(total_cases)
) %>%
select(fiscal_year, total_cases, difference_from_baseline, pct_change_from_baseline)
#NOTE TO RW: trends difference from article suggest missing data cleaning step, search "DATA CLEANING INVESTIGATION" in this code for what I'd like your thoughts on
#NOTE TO RW: Is the difference from baseline and pct change from baseline helpful? Or would you recommend changing it to year to year difference/pct change?
yearly_data_sum %>%
mutate(
pct_change_from_baseline = ((average_payout - first(average_payout))/first(average_payout)) * 100,
difference_from_baseline = average_payout - first(average_payout)
) %>%
select(fiscal_year, average_payout, difference_from_baseline, pct_change_from_baseline)
yearly_data_sum %>%
filter(fiscal_year == 2017) %>%
select(fiscal_year, number_of_cases_over_one_million)
yearly_data_sum %>%
filter(fiscal_year == 2007 | fiscal_year == 2017) %>%
mutate(decade_change_multiplier = number_of_cases_over_one_million/
first(number_of_cases_over_one_million)) %>%
select(fiscal_year, number_of_cases_over_one_million, decade_change_multiplier)
#NOTE TO RW: For clarities sake should I remove the "Decade cahnge multiplier" because maybe it implies that 2007 was the same as 1997. And instead just do a math operation outside the dataframe.
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(tidyverse)
library(rio)
library(lubridate)
library(janitor)
#import data function
payouts <- rio::import('payouts.xlsx',
skip=1, header=TRUE)
#The above skip/header functions deletes the wonky top row, make the second row into headers
#See AI Disclosure I (for import)
payouts <- payouts %>%
slice_head(n=18701) %>%
clean_names()
#the bottom two rows in the csv are one blank row and then a row with a grand total, this will screw up the data. Slice_head means filtering by that number of rows, I tried to use slice_tail(n=2) but it gave me JUST the bottom two rows, so I did total rows minus 2 and slice_head.
##Note to RW if you have a suggestion on a more succinct/elegant way to achieve the above function please let us know
#Import function for department totals which was brought into DAVIDS section from BEX's code to load department totals from comparison was deleted because it was from the output folder rather than the input folder.
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) %>%
arrange(fiscal_year)
# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>%
filter(fiscal_year == 2017)
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE),
average_payout = mean(amount, na.rm=TRUE),
#then filter the rows in the source table to rows with the amount column has a value over 1 million then count how many rows each fiscal_year value has that meet that condition for a column in the pivot table
#*SEE AI DISCLOSURE 2.1, for troubleshooting*
number_of_cases_over_one_million = sum(amount >= 1000000, na.rm=TRUE)
) %>%
arrange(fiscal_year)
yearly_data_sum %>%
#SEE AI DISCLOSURE 2.2 for troubleshooting of the following line only (if I am using AI to troubleshoot too much please let me know, for the record I am attempting, then googling, then if I cant find/understand what I've found I go to CLAUDE)
mutate(
pct_change_from_baseline = ((total_cases - first(total_cases))/
first(total_cases)) * 100,
difference_from_baseline = total_cases - first(total_cases)
) %>%
select(fiscal_year, total_cases, difference_from_baseline, pct_change_from_baseline)
#NOTE TO RW: trends difference from article suggest missing data cleaning step, search "DATA CLEANING INVESTIGATION" in this code for what I'd like your thoughts on
#NOTE TO RW: Is the difference from baseline and pct change from baseline helpful? Or would you recommend changing it to year to year difference/pct change?
yearly_data_sum %>%
mutate(
pct_change_from_baseline = ((average_payout - first(average_payout))/first(average_payout)) * 100,
difference_from_baseline = average_payout - first(average_payout)
) %>%
select(fiscal_year, average_payout, difference_from_baseline, pct_change_from_baseline)
yearly_data_sum %>%
filter(fiscal_year == 2017) %>%
select(fiscal_year, number_of_cases_over_one_million)
yearly_data_sum %>%
filter(fiscal_year == 2007 | fiscal_year == 2017) %>%
mutate(decade_change_multiplier = number_of_cases_over_one_million/
first(number_of_cases_over_one_million)) %>%
select(fiscal_year, number_of_cases_over_one_million, decade_change_multiplier)
#NOTE TO RW: For clarities sake should I remove the "Decade cahnge multiplier" because maybe it implies that 2007 was the same as 1997. And instead just do a math operation outside the dataframe.
#Note to RW, you said you didn't understand the Decade Change Multiplier. Its just saying there were 5 times as many cases in 2017 than there were 10 years before. 30/6=5, thats all.
lapd_cases <- payouts |>
filter(str_detect(department, 'Police'))
lapd_cases |>
arrange(desc(amount))
View(lapd_cases)
department_totals <- payouts |>
group_by(department) |>
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(desc(total_amount))
department_totals
# Geting total for all departments
overall_total <- sum(payouts$amount, na.rm = TRUE)
# Calculating LAPD totals and percentage
lapd_summary <- department_totals |>
filter(str_detect(department, 'Police')) |>
summarise(
total_lapd_cases = sum(total_cases),
total_lapd_amount = sum(total_amount),
percentage_of_total = (sum(total_amount) / overall_total) * 100
)
lapd_summary
View(department_totals)
View(department_totals)
# Show top 10 departments by payout amount
department_totals |>
mutate(percentage_of_total = (total_amount / overall_total) * 100) |>
arrange(desc(total_amount))
# total LAPD cases by year
lapd_cases |>
group_by(fiscal_year) |>
summarise(
cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(fiscal_year)
# total LAPD cases by year
lapd_cases_by_year<-lapd_cases |>
group_by(fiscal_year) |>
summarise(
cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(fiscal_year)
lapd_cases_by_year
lapd_cases_by_year %>%
sum(cases)
# total LAPD cases by year
lapd_cases |>
group_by(fiscal_year) |>
summarise(
cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(fiscal_year)
# total LAPD cases by year
lapd_cases |>
group_by(fiscal_year) |>
summarise(
cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(fiscal_year)
department_totals(department="Police Department")
# total LAPD cases by year
lapd_cases |>
group_by(fiscal_year) |>
summarise(
cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(fiscal_year)
department_totals(total_cases, department="Police Department")
# total LAPD cases by year
lapd_cases |>
group_by(fiscal_year) |>
summarise(
cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) |>
arrange(fiscal_year)
department_totals %>%
filter(department=="Police Department")
false_imprisonment_cases <- lapd_cases %>%
filter(str_detect(case_type, 'False Arrest/Detention/Imprisonment')) |>
arrange(desc(amount))
View(false_imprisonment_cases)
false_imprisonment_cases <- lapd_cases %>%
filter(str_detect(case_type, 'False Arrest/Detention/Imprisonment')) |>
arrange(desc(amount))
View(false_imprisonment_cases)
yearly_data_sum <- payouts %>%
group_by(fiscal_year) %>%
summarise(
total_cases = n(),
total_amount = sum(amount, na.rm = TRUE)
) %>%
arrange(fiscal_year)
# Check 2017 total (last budget year when article was published in 2018)
yearly_data_sum %>%
filter(fiscal_year == 2017)
View(yearly_data_sum)
